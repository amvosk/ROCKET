{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c469c20-0683-4547-8735-fa34479e5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62297829-717e-433d-892f-422675284bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda:1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "torch.set_default_tensor_type('torch.cuda.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb620c7-5caf-4fe0-9809-7999b95c2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_dataset = 'Univariate_arff/'\n",
    "# path_dataset_csv = path_dataset + 'dataset.csv'\n",
    "# dataset85 = 'dataset85.txt'\n",
    "\n",
    "# dataset_names = list(pd.read_csv(path_dataset_csv, encoding='ISO-8859-1', header=None)[:85][1].values)\n",
    "# with open(dataset85, 'w') as f:\n",
    "#     f.write('\\n'.join(dataset_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1297d246-cda8-4eed-ad81-031be4e3eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_dataset, dataset_name):\n",
    "\n",
    "    training_data = np.loadtxt(f\"{path_dataset}/{dataset_name}/{dataset_name}_TRAIN.txt\")\n",
    "    Y_training_, X_training = training_data[:, 0].astype(np.int32), training_data[:, 1:]\n",
    "\n",
    "    test_data = np.loadtxt(f\"{path_dataset}/{dataset_name}/{dataset_name}_TEST.txt\")\n",
    "    Y_test_, X_test = test_data[:, 0].astype(np.int32), test_data[:, 1:]\n",
    "\n",
    "    classes_old = np.unique(Y_training_)\n",
    "    n_classes = classes_old.shape[0]\n",
    "    \n",
    "    Y_training = np.zeros(Y_training_.shape[0]).astype(np.int32)\n",
    "    Y_test = np.zeros(Y_test_.shape[0]).astype(np.int32)\n",
    "    for i, c in enumerate(classes_old):\n",
    "        Y_training[Y_training_ == c] = i\n",
    "        Y_test[Y_test_ == c] = i\n",
    "\n",
    "    return X_training, Y_training, X_test, Y_test, n_classes, classes_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1df864-a3ad-4459-a1d2-b4c9ca2dda0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e480e577-7f5d-4032-8af9-cab10134c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFeatures(nn.Module):\n",
    "    kernel_length = torch.tensor([7, 9, 11], dtype=torch.float)\n",
    "    weight = None\n",
    "    bias = None\n",
    "    dilation = None\n",
    "    padding = None\n",
    "    \n",
    "    def __init__(self, n_kernels, ts_length, random_seed=0):\n",
    "        super(RandomFeatures, self).__init__()\n",
    "        \n",
    "        self.n_kernels = n_kernels\n",
    "        self.ts_length = ts_length\n",
    "        self.random_seed = random_seed * n_kernels\n",
    "        self.create_kernels(self.random_seed)\n",
    "        \n",
    "    def create_kernels(self, random_seed=0):\n",
    "        torch.manual_seed(random_seed)\n",
    "        kernel_length_indices = torch.multinomial(self.kernel_length, self.n_kernels, replacement=True)\n",
    "        kernel_lengths = self.kernel_length[kernel_length_indices].to(torch.long)\n",
    "        weight_distributions_ = [torch.normal(mean=0, std=1, size=(kernel_length,), dtype=torch.double) for kernel_length in kernel_lengths]\n",
    "        weight_distributions = [weight_distribution - weight_distribution.mean() for weight_distribution in weight_distributions_]\n",
    "        self.weight = [weight_distribution.unsqueeze(0).unsqueeze(0) for weight_distribution in weight_distributions]\n",
    "        \n",
    "        self.bias = torch.rand(size=(self.n_kernels,), dtype=torch.double).reshape(-1,1) * 2 - 1\n",
    "        A = math.log(self.ts_length-1) - torch.log(kernel_lengths-1)\n",
    "        s = torch.rand(size=(self.n_kernels,)) * A\n",
    "        self.dilation = torch.floor(2**s)\n",
    "        self.padding = ((self.dilation * (kernel_lengths-1) / 2) * torch.randint(2, size=(self.n_kernels,)))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = einops.rearrange(x, 'b t -> b 1 t')\n",
    "        features = torch.empty(size=(batch_size, self.n_kernels, 2))\n",
    "        \n",
    "        for i in range(self.n_kernels):\n",
    "            ts_convolved = nn.functional.conv1d(\n",
    "                x, weight=self.weight[i], bias=self.bias[i], \n",
    "                padding=int(self.padding[i].item()), dilation=int(self.dilation[i].item())\n",
    "            )\n",
    "            ts_convolved = ts_convolved.squeeze(1)\n",
    "\n",
    "            ts_convolved_max = torch.max(ts_convolved, dim=1).values\n",
    "            features[:,i,0] = ts_convolved_max\n",
    "\n",
    "            ts_convolved_ppv = torch.mean((ts_convolved > 0).to(torch.float), dim=1)\n",
    "            features[:,i,1] = ts_convolved_ppv\n",
    "        \n",
    "        features = einops.rearrange(features, 'b s f -> b (s f)')\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "050a6210-e83e-437f-b524-fb96099a5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, mc_dropout=0.3, random_seed=0):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.mc_dropout = mc_dropout\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        torch.manual_seed(self.random_seed)\n",
    "        self.linear = nn.Linear(n_features, n_classes)\n",
    "        self.input_dropout = nn.Dropout(p=self.mc_dropout)\n",
    "    \n",
    "    def update_dropout(self, mc_dropout=0.3):\n",
    "        self.input_dropout = nn.Dropout(p=mc_dropout)\n",
    "    \n",
    "    def forward(self, x, use_dropout=False):\n",
    "        if use_dropout:\n",
    "            x = self.input_dropout(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "921c79e0-f225-4932-ac16-8501759bbf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.train_loss = []\n",
    "\n",
    "    def fit(self, x, y, nepoch=1, lr=1e-3, tol=None, counter_thr = 10):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [{'params':param, 'lr':lr} for param in self.model.parameters()]\n",
    "        )\n",
    "        counter = 0\n",
    "        for epoch in range(nepoch):\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            logits = self.model(x)\n",
    "            loss = self.criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.train_loss.append(loss.detach().cpu().numpy().item())\n",
    "            if tol is not None and epoch > 1:\n",
    "                if abs(self.train_loss[-1] - self.train_loss[-2]) <= tol:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    counter = 0\n",
    "            if counter >= counter_thr:\n",
    "                break\n",
    "            \n",
    "    def predict_logits(self, x):\n",
    "        self.model.eval()\n",
    "        logits_unscaled = self.model(x)\n",
    "        logits = self.logsoftmax(logits_unscaled)\n",
    "        return logits\n",
    "    \n",
    "    def predict_mc_logits(self, x, n_samples_mc=100):\n",
    "        batch_size = x.size(0)\n",
    "        self.model.train()\n",
    "        logits_unscaled = torch.empty(size=(batch_size, self.model.n_classes, n_samples_mc))\n",
    "        for i in range(n_samples_mc):\n",
    "            logits_unscaled_ = self.model(x, use_dropout=True)\n",
    "            logits_unscaled[:,:,i] = logits_unscaled_.detach().clone()\n",
    "        logits_sum = self.logsoftmax(logits_unscaled)\n",
    "        logits = torch.logsumexp(logits_sum, dim=2) - math.log(n_samples_mc)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26968057-2feb-4132-b6af-cd5be510af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, targets):\n",
    "    # probs = np.exp(logits.detach().cpu().numpy())\n",
    "    probs = torch.exp(logits)\n",
    "    correspondence = (torch.argmax(probs, axis=1) == targets).to(torch.float)\n",
    "    return correspondence.mean().detach().cpu().numpy().item()\n",
    "\n",
    "def negative_loglikelihood(logits, targets):\n",
    "    nll = nn.NLLLoss()\n",
    "    return nll(logits, targets).detach().cpu().numpy().item()\n",
    "\n",
    "def brier_score(logits, targets, n_classes):\n",
    "    probs = torch.exp(logits)\n",
    "    targets_one_hot = nn.functional.one_hot(targets, num_classes=n_classes)\n",
    "    square_errors = (probs - targets_one_hot)**2\n",
    "    return square_errors.mean().detach().cpu().numpy().item()\n",
    "\n",
    "def expected_calibration_error(logits, targets, n_bins=10):\n",
    "    bins_low = torch.arange(n_bins) / n_bins\n",
    "    bins_high = (torch.arange(n_bins) + 1) / n_bins\n",
    "\n",
    "    probs = torch.exp(logits)\n",
    "    p_hat = torch.max(probs, dim=1).values\n",
    "    targets_hat = torch.argmax(probs, dim=1)\n",
    "    accuracy = (targets_hat == targets)\n",
    "    \n",
    "    calibration_errors = []\n",
    "    for i in range(n_bins):\n",
    "        mask = ((p_hat >= bins_low[i] if i==0 else p_hat > bins_low[i]) * (p_hat <= bins_high[i]))\n",
    "        if torch.any(mask):\n",
    "            bin_size = torch.sum(mask.to(torch.float)).item()\n",
    "            p_hat_masked = torch.mean(p_hat[mask].to(torch.float)).item()\n",
    "            accuracy_masked = torch.mean(accuracy[mask].to(torch.float)).item()\n",
    "            calibration_error = abs(accuracy_masked - p_hat_masked)\n",
    "            calibration_errors.append(calibration_error * bin_size / targets.size(0))\n",
    "    return np.sum(calibration_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a218a9-0d7b-410d-82e0-bb23a849310f",
   "metadata": {},
   "source": [
    "### Single Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6356d13d-cfce-48de-bf7b-4a57f3fca859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTrialEstimate():\n",
    "    def __init__(self, n_kernels, ts_length, n_classes, n_ece_bins, random_seed=0):\n",
    "        self.n_ece_bins = n_ece_bins\n",
    "        self.n_classes = n_classes\n",
    "        self.random_seed = random_seed\n",
    "        n_features = 2 * n_kernels\n",
    "        self.rf = RandomFeatures(n_kernels, ts_length, random_seed)\n",
    "        self.lr = LogisticRegression(n_features, n_classes, random_seed=random_seed)\n",
    "        self.fitted = False\n",
    "        self.trainer = None\n",
    "        \n",
    "    def fit(self, X, Y, nepoch=5000, lr=1e-4, tol=1e-4, counter_thr=300):\n",
    "        X = torch.tensor(X, dtype=torch.double)\n",
    "        Y = torch.tensor(Y, dtype=torch.long)\n",
    "        \n",
    "        features = self.rf(X).detach().clone()\n",
    "        self.trainer = Trainer(self.lr)\n",
    "        self.trainer.fit(features, Y, nepoch=nepoch, lr=lr, tol=tol, counter_thr=counter_thr)\n",
    "        self.fitted = True\n",
    "        \n",
    "    def score(self, X, Y):\n",
    "        assert self.fitted, 'model should be fitted'\n",
    "        X = torch.tensor(X, dtype=torch.double)\n",
    "        Y = torch.tensor(Y, dtype=torch.long)\n",
    "        \n",
    "        features = self.rf(X).detach().clone()\n",
    "        logits = self.trainer.predict_logits(features)\n",
    "        results = {}\n",
    "        results['acc'] = accuracy(logits, Y)\n",
    "        results['nll'] = negative_loglikelihood(logits, Y)\n",
    "        results['bs'] = brier_score(logits, Y, self.n_classes)\n",
    "        results['ece'] = expected_calibration_error(logits, Y, self.n_ece_bins)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44dca0d7-0425-4906-8136-fd5627926da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropoutEstimate(SingleTrialEstimate):\n",
    "    def __init__(self, n_kernels, ts_length, n_classes, n_ece_bins, random_seed=0,\n",
    "                 mc_dropout=0.01, n_samples_mc=100):\n",
    "        super(MCDropoutEstimate, self).__init__(n_kernels, ts_length, n_classes, n_ece_bins, random_seed)\n",
    "        self.mc_dropout = mc_dropout\n",
    "        self.n_samples_mc = n_samples_mc\n",
    "        \n",
    "    def score(self, X, Y):\n",
    "        assert self.fitted, 'model should be fitted'\n",
    "        X = torch.tensor(X, dtype=torch.double)\n",
    "        Y = torch.tensor(Y, dtype=torch.long)\n",
    "        \n",
    "        features = self.rf(X).detach().clone()\n",
    "        self.trainer.model.update_dropout(self.mc_dropout)\n",
    "        logits = self.trainer.predict_mc_logits(features, self.n_samples_mc)\n",
    "        results = {}\n",
    "        results['acc'] = accuracy(logits, Y)\n",
    "        results['nll'] = negative_loglikelihood(logits, Y)\n",
    "        results['bs'] = brier_score(logits, Y, self.n_classes)\n",
    "        results['ece'] = expected_calibration_error(logits, Y, self.n_ece_bins)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a789e27b-92ed-4cfe-acc8-e4c38d2a143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleEstimate():\n",
    "    def __init__(self, n_kernels, ts_length, n_classes, n_ece_bins, random_seed=0, \n",
    "                 n_random_features=10):\n",
    "        self.n_ece_bins = n_ece_bins\n",
    "        self.n_classes = n_classes\n",
    "        self.random_seed = random_seed + 1\n",
    "        self.n_random_features = n_random_features\n",
    "        n_features = 2 * n_kernels\n",
    "        self.rf = [RandomFeatures(n_kernels, ts_length, random_seed+i) for i in range(self.n_random_features)]\n",
    "        self.lr = [LogisticRegression(n_features, n_classes, random_seed=random_seed+i) for i in range(self.n_random_features)]\n",
    "        self.fitted = False\n",
    "        self.trainer = None\n",
    "        \n",
    "    def fit(self, X, Y, nepoch=5000, lr=1e-4, tol=1e-4, counter_thr=300):\n",
    "        X = torch.tensor(X, dtype=torch.double)\n",
    "        Y = torch.tensor(Y, dtype=torch.long)\n",
    "        self.trainer = [Trainer(lr) for lr in self.lr]\n",
    "        for i in range(self.n_random_features):\n",
    "            features = self.rf[i](X).detach().clone()\n",
    "            self.trainer[i].fit(features, Y, nepoch=nepoch, lr=lr, tol=tol, counter_thr=counter_thr)\n",
    "        self.fitted = True\n",
    "        \n",
    "    def score(self, X, Y):\n",
    "        assert self.fitted, 'model should be fitted'\n",
    "        X = torch.tensor(X, dtype=torch.double)\n",
    "        Y = torch.tensor(Y, dtype=torch.long)\n",
    "        \n",
    "        logits_ensemble = torch.empty(size=(X.shape[0], self.n_classes, self.n_random_features))\n",
    "        for i in range(self.n_random_features):\n",
    "            features = self.rf[i](X).detach().clone()\n",
    "            logits_ = self.trainer[i].predict_logits(features)\n",
    "            logits_ensemble[:,:,i] = logits_\n",
    "        logits_ensemble = torch.nn.LogSoftmax(dim=1)(logits_ensemble)\n",
    "        logits = torch.logsumexp(logits_ensemble, dim=2) - math.log(self.n_random_features)\n",
    "            \n",
    "        results = {}\n",
    "        results['acc'] = accuracy(logits, Y)\n",
    "        results['nll'] = negative_loglikelihood(logits, Y)\n",
    "        results['bs'] = brier_score(logits, Y, self.n_classes)\n",
    "        results['ece'] = expected_calibration_error(logits, Y, self.n_ece_bins)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "130d3841-d637-45f6-981e-5abd494df423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTEstimate(SingleTrialEstimate):\n",
    "    def __init__(self, n_kernels, ts_length, n_classes, n_ece_bins, random_seed=0,\n",
    "                 n_samples=10, style='full'):\n",
    "        super(FFTEstimate, self).__init__(n_kernels, ts_length, n_classes, n_ece_bins, random_seed)\n",
    "        self.n_samples = n_samples\n",
    "        self.style = style\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        assert self.fitted, 'model should be fitted'\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.double)\n",
    "        Y = torch.tensor(Y, dtype=torch.long)\n",
    "        \n",
    "        n_test = X.size(0)\n",
    "\n",
    "        X_fft = torch.fft.fft(X, dim=1)\n",
    "        X_fft_abs = np.abs(X_fft.detach().cpu().numpy())\n",
    "        X_fft_angle = np.angle(X_fft.detach().cpu().numpy())\n",
    "\n",
    "        X_fft_angle_sampled = []\n",
    "        l = (X_fft_angle.shape[1]-1)//2\n",
    "        \n",
    "        if self.style == 'odd':\n",
    "            amp = X_fft_abs[:,1:l+1][:,::2]\n",
    "        elif self.style == 'even':\n",
    "            amp = X_fft_abs[:,1:l+1][:,1::2]\n",
    "        elif self.style == 'full':\n",
    "            amp = X_fft_abs[:,1:l+1]\n",
    "        elif self.style == 'unif':\n",
    "            amp = np.ones_like(X_fft_abs)[:,1:l+1]\n",
    "        p = amp / np.sum(amp, axis=1, keepdims=True)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            X_fft_angle_ = np.copy(X_fft_angle)\n",
    "            for j in range(n_test):\n",
    "     \n",
    "                if self.style == 'odd':\n",
    "                    index = np.random.choice(np.arange(1, l+1)[::2], size=1, p=p[j])\n",
    "                elif self.style == 'even':\n",
    "                    index = np.random.choice(np.arange(1, l+1)[1::2], size=1, p=p[j])\n",
    "                elif self.style == 'full':\n",
    "                    index = np.random.choice(np.arange(1, l+1), size=1, p=p[j])\n",
    "                elif self.style == 'unif':\n",
    "                    index = np.random.choice(np.arange(1, l+1), size=1, p=p[j])\n",
    "                phase = (np.random.rand(1) - 0.5) * 2 * np.pi\n",
    "                X_fft_angle_[j,index] = phase\n",
    "                X_fft_angle_[j,-index] = - phase\n",
    "            X_fft_angle_sampled.append(X_fft_angle_)\n",
    "\n",
    "        X_fft_angle_sampled = np.stack(X_fft_angle_sampled)\n",
    "        X_fft_sampled = torch.tensor(X_fft_abs*np.exp(1j*X_fft_angle_sampled))\n",
    "        X_sampled = torch.real(torch.fft.ifft(X_fft_sampled, dim=2))\n",
    "        \n",
    "        \n",
    "        logits_sampled = torch.empty(n_test, self.n_classes, self.n_samples)\n",
    "        for i in range(self.n_samples):\n",
    "            X_ = X_sampled[i]\n",
    "            features_ = self.rf(X_).detach().clone()\n",
    "            logits_ = self.trainer.predict_logits(features_).detach().clone()\n",
    "            logits_sampled[:,:,i] = logits_\n",
    "\n",
    "        logits_sampled = torch.nn.LogSoftmax(dim=1)(logits_sampled)\n",
    "        logits = torch.logsumexp(logits_sampled, dim=2) - math.log(self.n_samples)\n",
    "        \n",
    "        results = {}\n",
    "        results['acc'] = accuracy(logits, Y)\n",
    "        results['nll'] = negative_loglikelihood(logits, Y)\n",
    "        results['bs'] = brier_score(logits, Y, self.n_classes)\n",
    "        results['ece'] = expected_calibration_error(logits, Y, self.n_ece_bins)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94bcda5-6be9-46c0-9b41-ff206fed564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = 'Univariate_arff/'\n",
    "dataset85 = 'dataset85.txt'\n",
    "with open(dataset85, 'r') as f:\n",
    "    dataset_names = list(map(str.strip, f.readlines()))\n",
    "\n",
    "n_datasets = len(dataset_names)\n",
    "\n",
    "n_kernels = 10000\n",
    "random_seed = 0\n",
    "n_ece_bins = 10\n",
    "\n",
    "n_samples = 100\n",
    "mc_dropout = 0.01\n",
    "n_samples_mc = 1000\n",
    "n_random_features = 10\n",
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2d9040-7d0c-4c9d-bc87-c26dbd448b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ElectricDevices'\n",
    "X_training, Y_training, X_test, Y_test, n_classes, classes_old = load_dataset(path_dataset, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa8ca8d8-b90b-4938-b4bd-30252744840f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06665996710459392 ste {'acc': 0.9000000357627869, 'nll': 0.45585725154843065, 'bs': 0.10424116029228159, 'ece': 0.12803682088851928}\n",
      "0.07454656759897868 mcde {'acc': 0.9000000357627869, 'nll': 0.45497639846637783, 'bs': 0.10437044272640786, 'ece': 0.12827165126800538}\n",
      "0.6264220317204793 ee {'acc': 0.9000000357627869, 'nll': 0.4515085405461571, 'bs': 0.1053959328434176, 'ece': 0.14237320125102998}\n",
      "2.660733699798584 ffteO {'acc': 0.9000000357627869, 'nll': 0.44926954472045366, 'bs': 0.12198390299057321, 'ece': 0.208628910779953}\n",
      "2.6010375658671063 ffteE {'acc': 0.9000000357627869, 'nll': 0.49593707317142516, 'bs': 0.14892690024992222, 'ece': 0.2514934420585632}\n",
      "2.6497342308362324 ffteF {'acc': 0.9000000357627869, 'nll': 0.4728118235254476, 'bs': 0.13161422635499892, 'ece': 0.22857072353363037}\n",
      "2.6488574584325155 ffteU {'acc': 0.9000000357627869, 'nll': 0.452806532166381, 'bs': 0.10393166652685062, 'ece': 0.12664009928703307}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = dataset_names[4]\n",
    "X_training, Y_training, X_test, Y_test, n_classes, classes_old = load_dataset(path_dataset, dataset_name)\n",
    "ts_length = X_training.shape[1]\n",
    "\n",
    "estimators_names = ['ste', 'mcde', 'ee', 'ffteO', 'ffteE', 'ffteF', 'ffteU']\n",
    "\n",
    "for estimators_name in estimators_names:\n",
    "    if estimators_name == 'ste':\n",
    "        est = SingleTrialEstimate(n_kernels, ts_length, n_classes, n_ece_bins, random_seed)\n",
    "    elif estimators_name == 'mcde':\n",
    "        est = MCDropoutEstimate(n_kernels, ts_length, n_classes, n_ece_bins, random_seed, mc_dropout, n_samples_mc)\n",
    "    elif estimators_name == 'ee':\n",
    "        est = EnsembleEstimate(n_kernels, ts_length, n_classes, n_ece_bins, random_seed, n_random_features)\n",
    "    elif estimators_name == 'ffteO':\n",
    "        est = FFTEstimate(n_kernels, ts_length, n_classes, n_ece_bins, random_seed, n_samples, 'odd')\n",
    "    elif estimators_name == 'ffteE':\n",
    "        est = FFTEstimate(n_kernels, ts_length, n_classes, n_ece_bins, random_seed, n_samples, 'even')\n",
    "    elif estimators_name == 'ffteF':\n",
    "        est = FFTEstimate(n_kernels, ts_length, n_classes, n_ece_bins, random_seed, n_samples, 'full')\n",
    "    elif estimators_name == 'ffteU':\n",
    "        est = FFTEstimate(n_kernels, ts_length, n_classes, n_ece_bins, random_seed, n_samples, 'unif')\n",
    "\n",
    "    t1 = time.time()\n",
    "    path_save = 'results/' + dataset_name + '_' + estimators_name + '.h5'\n",
    "    est.fit(X_training, Y_training)\n",
    "    result = est.score(X_test, Y_test)\n",
    "    print((time.time() - t1)/60, estimators_name, result)\n",
    "    \n",
    "    # with h5py.File(path_save, 'w') as file:\n",
    "    #     for k, v in result.items():\n",
    "    #         file.attrs[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d6ae84-c514-4f83-8018-21d47cddd7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d4221-ce44-41ce-bcd8-69eaee23459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_dataset = 'Univariate_arff/'\n",
    "# path_dataset_csv = path_dataset + 'dataset.csv'\n",
    "# dataset85 = 'dataset85.txt'\n",
    "\n",
    "# dataset_names = list(pd.read_csv(path_dataset_csv, encoding='ISO-8859-1', header=None)[:85][1].values)\n",
    "# with open(dataset85, 'w') as f:\n",
    "#     f.write('\\n'.join(dataset_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1151896b-0e9c-4822-8206-7fb33dfcf9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07b910-05f3-4e07-9ad2-d88eb7bdd53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc7320-7d57-4914-9d1f-f018191e3795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87545e3-ca50-43d6-9241-4d0769237402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e61b9-33d3-45dd-af99-6ea054b375ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
